
 Arguments:
action_rate: 0.1
agent_mode: act
batch_size: 32
context_len: 100
contextual_embedding: glove
dense_dim: 256
dis_dim: 100
display_training_result: 1
domain: win2k
dropout: 0.5
epochs: 5
exploration_decay_steps: 1000
exploration_rate_end: 0.1
exploration_rate_start: 1
exploration_rate_test: 0.0
filter_act_ind: 1
gamma: 0.9
gui_mode: False
learning_rate: 0.001
load_replay: 0
load_weights: False
model_dim: 50
num_actions: 2
num_filters: 32
num_words: 500
object_rate: 0.07
optimizer: adam
positive_rate: 0.9
priority: 1
random_play: 0
replay_size: 50000
result_dir: results/win2k_act_glove_pos
reward_assign: [1, 2, 3]
reward_base: 50.0
save_replay: 0
save_replay_name: data/saved_replay_memory.pkl
save_replay_size: 1000
save_weights: True
stacked_embeddings: StackedEmbeddings [/home/nfs/smiglani/.flair/embeddings/glove.gensim]
start_epoch: 0
stop_epoch_gap: 5
tag_dim: 100
target_steps: 5
test_steps: 15500
train_episodes: 50
train_frequency: 1
train_repeat: 1
train_steps: 49000
use_act_rate: 1
valid_steps: 12500
word2vec: <gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f30c13b8fd0>
word_dim: 100



Summary:
total_ecs: 0	 right_ecs: 0	 tagged_ecs: 0
total_act: 204	 right_act: 204	 tagged_act: 1058
acc: 0.192817	 rec: 1.000000	 pre: 0.192817	 f1: 0.323296

cumulative reward: -23519.255000	 average reward: -22.229920


 Best f1 value: {'rec': [0.0, 1.0], 'pre': [0.0, 0.19281663516068054], 'f1': [0.0, 0.32329635499207604], 'rw': [0.0, -22.229919659735305]}  best epoch: 0


Summary:
total_ecs: 0	 right_ecs: 0	 tagged_ecs: 0
total_act: 204	 right_act: 204	 tagged_act: 1058
acc: 0.192817	 rec: 1.000000	 pre: 0.192817	 f1: 0.323296

cumulative reward: -23519.255000	 average reward: -22.229920


Summary:
total_ecs: 0	 right_ecs: 0	 tagged_ecs: 0
total_act: 204	 right_act: 204	 tagged_act: 1058
acc: 0.192817	 rec: 1.000000	 pre: 0.192817	 f1: 0.323296

cumulative reward: -23519.255000	 average reward: -22.229920


Summary:
total_ecs: 0	 right_ecs: 0	 tagged_ecs: 0
total_act: 204	 right_act: 204	 tagged_act: 1058
acc: 0.192817	 rec: 1.000000	 pre: 0.192817	 f1: 0.323296

cumulative reward: -23519.255000	 average reward: -22.229920


Summary:
total_ecs: 0	 right_ecs: 0	 tagged_ecs: 0
total_act: 204	 right_act: 204	 tagged_act: 1058
acc: 0.192817	 rec: 1.000000	 pre: 0.192817	 f1: 0.323296

cumulative reward: -23519.255000	 average reward: -22.229920


Summary:
total_ecs: 0	 right_ecs: 0	 tagged_ecs: 0
total_act: 204	 right_act: 204	 tagged_act: 1058
acc: 0.192817	 rec: 1.000000	 pre: 0.192817	 f1: 0.323296

cumulative reward: -23519.255000	 average reward: -22.229920


Summary:
total_ecs: 0	 right_ecs: 0	 tagged_ecs: 0
total_act: 204	 right_act: 204	 tagged_act: 1058
acc: 0.192817	 rec: 1.000000	 pre: 0.192817	 f1: 0.323296

cumulative reward: -23519.255000	 average reward: -22.229920


Summary:
total_ecs: 0	 right_ecs: 0	 tagged_ecs: 0
total_act: 204	 right_act: 204	 tagged_act: 1058
acc: 0.192817	 rec: 1.000000	 pre: 0.192817	 f1: 0.323296

cumulative reward: -23519.255000	 average reward: -22.229920


Summary:
total_ecs: 0	 right_ecs: 0	 tagged_ecs: 0
total_act: 204	 right_act: 204	 tagged_act: 1058
acc: 0.192817	 rec: 1.000000	 pre: 0.192817	 f1: 0.323296

cumulative reward: -23519.255000	 average reward: -22.229920


Summary:
total_ecs: 0	 right_ecs: 0	 tagged_ecs: 0
total_act: 204	 right_act: 204	 tagged_act: 1058
acc: 0.192817	 rec: 1.000000	 pre: 0.192817	 f1: 0.323296

cumulative reward: -23519.255000	 average reward: -22.229920


 training process:
{'rec': [0.0, 1.0], 'pre': [0.0, 0.19281663516068054], 'f1': [0.0, 0.32329635499207604], 'rw': [0.0, -22.229919659735305]}

rec: [1.0]
pre: [0.19281663516068054]
f1: [0.32329635499207604]
rw: [-22.229919659735305]

Avg f1: 0.32329635499207604  Avg reward: -22.229919659735305
